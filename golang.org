* Golang in action
** golang tools
*** build windows exe file on linux platform
	GOOS=windows GOARCH=amd64  go build ~/go/src/xchg.ai/sse/smalltest
    上面这个指令在Linux平台上可以编译出windows可执行文件
*** clean build out files
    go clean
*** build and running the excute file
    go run main.go xxx
    xxx 是参数
*** go env
    察看GO开发环境，可以为我们排查错误提供帮助
*** go install
    如果编译生成的是可执行文件，安装在$GOPATH/bin目录下
    如果编译生成的是可引用的库，安装在$GOPATH/pkg目录下
*** go fmt
*** go vet
*** go test
** golang doc
*** 终端查看文档：
	在当前目录下执行go doc, 输出当前目录下的文档信息；
    go doc packageName 列出这个包的信息，包括文档，方法，结构体等
    go doc json
    go doc json.Decoder
	go doc json.Decoder.Decode
    可以一步步缩小查找范围
*** 在线浏览文档：
*** 生成自己的文档（添加例子）：
* Golang test and benchmark
   func Test_XXXX(b *testing.B) {
	  //this is the testing code
   }

   //gen profile
   go test -bench=. -benchmem -memprofile memprofile.out -cpuprofile profile.out

   //check profile
   go tool pprof profile.out
* Golang web development
** Http Server
*** 服务器的主要职责：	
    Process dynamic requests: [处理动态请求] 
	Process incoming requests from users who browse the website, log into their accounts or post images.
    Serve static assets: [静态文件服务]
	Serve JavaScript, CSS and images to browsers to create a dynamic experience for the user.
    Accept connections: [网络连接管理]
	The HTTP Server must listen on a specific port to be able to accept connections from the internet.

** 路由器
   [[file:~/PlayGround/Golang/PlayWeb/hello_router.go][play with the gorilla router now!]]

** Connet with MySQL database
   Install a mysql server on a linux mechine
      
* Golang pprof
** profiling a golang program
  #+BEGIN_SRC
package main

import (
	"bufio"
	"flag"
	"fmt"
	"log"
	"os"
	"runtime"
	"runtime/pprof"
	"strconv"
	"sync"
)

const (
	DIRPATH    string = "./"
	FILENAME   string = "file"
	FILESUFFIX string = ".txt"
)

var fileIndexes []int = []int{1, 2, 3, 4}

var cpuprofile = flag.String("cpuprofile", "", "write cpu profile to `file`")
var memprofile = flag.String("memprofile", "", "write memory profile to `file`")

func main() {
	var wg sync.WaitGroup

	flag.Parse()
	if *cpuprofile != "" {
		f, err := os.Create(*cpuprofile)
		if err != nil {
			log.Fatal("could not create CPU profile: ", err)
		}
		defer f.Close() // error handling omitted for example
		if err := pprof.StartCPUProfile(f); err != nil {
			log.Fatal("could not start CPU profile: ", err)
		}
		defer pprof.StopCPUProfile()
	}

	for _, i := range fileIndexes {
		filePath := DIRPATH + FILENAME + strconv.Itoa(i) + FILESUFFIX

		wg.Add(1)
		go func() {
			defer wg.Done()

			file, err := os.Open(filePath)
			if err != nil {
				panic(err)
			}

			defer file.Close()

			scanner := bufio.NewScanner(file)
			for scanner.Scan() {
				fmt.Println(scanner.Text())
			}
		}()
	}

	wg.Wait()

	if *memprofile != "" {
		f, err := os.Create(*memprofile)
		if err != nil {
			log.Fatal("could not create memory profile: ", err)
		}
		defer f.Close() // error handling omitted for example
		runtime.GC()    // get up-to-date statistics
		if err := pprof.WriteHeapProfile(f); err != nil {
			log.Fatal("could not write memory profile: ", err)
		}
	}
}
  #+END_SRC
  the privious code show how to populate pprof in a golang program  

  after you build the project, the following command will generate the profile:
  =test_profile -cpuprofile test_profile.prof=

  then you can use the profile to invesgate the cpu usage sketch of the program:
  =go tool pprof test_pprof test_pprof.prof=

  when you in the pprof mode, you can type:
  =web=
  generate a graph of the program.
  
** using profile analyse routines stack
*** using net/http/pprof
#+BEGIN_SRC
package main

import (
	"fmt"
	"net/http"
	_ "net/http/pprof"
)

func main() {
	ip := "0.0.0.0:6060"
	if err := http.ListenAndServe(ip, nil); err != nil {
		fmt.Println("start pprof failed on %s\n", ip)
	}
}
#+END_SRC

open browser, and input http://localhost:6060/debug/pprof/
you will get a page.

use command line get profile message:
# 下载cpu profile，默认从当前开始收集30s的cpu使用情况，需要等待30s
go tool pprof http://localhost:6060/debug/pprof/profile                 # 30-second CPU profile
go tool pprof http://localhost:6060/debug/pprof/profile?seconds=120     # wait 120s

# 下载heap profile
go tool pprof http://localhost:6060/debug/pprof/heap      # heap profile

# 下载goroutine profile
go tool pprof http://localhost:6060/debug/pprof/goroutine # goroutine profile

# 下载block profile
go tool pprof http://localhost:6060/debug/pprof/block     # goroutine blocking profile

# 下载mutex profile
go tool pprof http://localhost:6060/debug/pprof/mutex

*** using pprof get heap message
	go tool pprof http://localhost:6060/debug/pprof/heap
    top
    list
    traces
*** memory leak:
**** how to know memory leak? [如何知道程序中有内存泄露呢？]
     1. write your own batch file monitor the memory usage of your program:
#+BEGIN_SRC
#!/bin/bash
prog_name="demo1"
prog_mem=$(pidstat -r -u -h -C $prog_name |awk 'NR==4{print $12}')
time=$(date "+%Y-%m-%d %H:%M:%S")
echo $time"\tmemory(Byte)\t"$prog_mem >>~/record/prog_mem.log
#+END_SRC
        or you can use top | grep "your_programe_name" check memory useage
     2. use pprof
        a leak deamon:
#+BEGIN_SRC
package main

import (
	"fmt"
	"net/http"
	_ "net/http/pprof"
	"os"
	"time"
)

func main() {
	go func() {
		ip := "0.0.0.0:6060"
		if err := http.ListenAndServe(ip, nil); err != nil {
			fmt.Printf("start pprof failed on %s\n", ip)
			os.Exit(1)
		}
	}()

	outChan := make(chan int)

	//dead code, never read from the channel
	go func() {
		if false {
			<-outChan
		}
		select {}
	}()

	//spwan 10 routines per second,
	tick := time.Tick(time.Second / 10)
	i := 0
	for range tick {
		i++
		fmt.Println(i)
		alloc1(outChan)
	}
}

func alloc1(outChan chan<- int) {
	go alloc2(outChan)
}

func alloc2(outChan chan<- int) {
	go func() {
		defer fmt.Println("alloc-fm exit")
		//alloc some memory
		buf := make([]byte, 1024*1024*10)
		_ = len(buf)
		fmt.Println("alloc done")

		outChan <- 1
	}()
}
#+END_SRC
        in the privious code, main routine create 10 routine every second,
        beacause each routine is wait on "outChan<-1", so the allocated memory
        can not be freed.

		we use "go tool pprof" get the infomation of goroutines:

		go tool pprof http://localhost:6060/debug/pprof/goroutine
		
		do privious command two time, get:
		/Users/hjiang/pprof/pprof.goroutine.001.pb.gz
        /Users/hjiang/pprof/pprof.goroutine.002.pb.gz

		then, enter:
		go tool pprof -base /Users/hjiang/pprof/pprof.goroutine.001.pb.gz /Users/hjiang/pprof/pprof.goroutine.002.pb.gz
		when we input "top" command:
		[[file:./graph/leak_demo.png]]
		use 001.pb.gz as the base, we can see 002.pb.gz's routine number has increased 67!

**** how to locate where the leak happens?
	 1. use Web browser
		run the leak golang program, enter this address to the web browser:
        http://localhost:6060/debug/pprof/goroutine?debug=1
		result:
		[[file:./graph/leak_demo_web1.png]]
		
		total 1589: the total number of goroutine
        1584@xxxx : the total number of goroutine waiting in this place
        main.go:52 : the problem program line

		52: outChan <- 1 
        in the 52th line of the program, we write a value into an unbuffered channel,
        which will never be read out. So every goroutine write into this channel will
        wait forever, this is a leaking point!

		let's enter another line into the web browser:
		http://localhost:6060/debug/pprof/goroutine?debug=2
		result:
		[[file:./graph/leak_demo_web2.png]]

		[[file:./graph/leak_demo_web3.png]]
		
		you can also locate where is the leaking point!
	 2. use command line
		go tool pprof http://localhost:6060/debug/pprof/goroutine
		reuslt:
		[[file:./graph/leak_demo_web4.png]]
		
		a. enter top: find the routines number
        b. enter traces: find the call stack
        c. list: list code

		[[file:./graph/leak_demo_web5.png]]
** pprof and gabage collection
*** when program create so many objects in a limited short time, look at the cpu and memory usage
#+BEGIN_SRC
package main

import (
	"fmt"
	"sync"
	"time"
)

type Student struct {
	Name   string
	Number uint32
}

func main() {
	for {
		createCrowds()
		time.Sleep(time.Millisecond * 10)
	}
}

func createCrowds() {
	var wg sync.WaitGroup
	for i := 0; i < 10; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()

			for i := 0; i < 10000000; i++ {
				_ = Student{
					Name:   "I do not need your education!",
					Number: 1,
				}
			}
		}()
	}
	wg.Wait()
	fmt.Println("create crowds finish!")
}
#+END_SRC
  The privious code generate 100 million "Student" object every 0.1 second;
let's watch the cpu and memory of this mechine:
  cpu:
  [[file:./graph/cpu.png]]
  mem:
  [[file:./graph/mem.png]]
  we can see an dramatic increase in cpu usage, but the memory usage is almost
not changed. because the gc is so busy, cost so many cpu usage. 
#+BEGIN_SRC
time.Sleep(time.Millisecond * 1)
#+END_SRC
  we intentional change the create speed! create the same number of objects in 0.01 second!
  cpu:
  [[file:./graph/cpu_0.01.png]]
  mem:
  [[file:./graph/mem_0.01.png]]
we can see cpu usage almost increase 25%; the we use pprof inspect program,
locate the key problem.
#+BEGIN_SRC
_ "net/http/pprof"

go func() {
		http.ListenAndServe("0.0.0.0:8005", nil)
	}()
#+END_SRC
go tool pprof -http=:1234 http://localhost:8005/debug/pprof/profile?seconds=30
30 seconds later we get a web page, click VIEW, then Flame Graph,:
[[file:./graph/flame_graph.png]]
we can see createCrowds function use the most cpu resource, then we use pprof watch heap infomation:
go tool pprof -http=:1234 http://localhost:8005/debug/pprof/heap
*** golang gc	
* Golang benchmark
  https://golang.org/pkg/testing/
* Golang concurrent pattern
** confinement [限定，不涉及同步原语]
   find some method or make a convetion to ensure that the information is only
   avaliable from one concurrent process(routine).
*** Ad hoc confinement
#+BEGIN_SRC
package main

import "fmt"

var data = make([]int, 4)

func main() {
	loopData := func(handleData chan<- int) {
		defer close(handleData)
		for i := range data {
			handleData <- data[i]
		}
	}

	handleData := make(chan int)
	go loopData(handleData)

	for num := range handleData {
		fmt.Println(num)
	}
}	
#+END_SRC

   in previous code snippet, we can see we only touch "data" slice in
the loopData routine. we have the criteria "in any single timestamp,
there is only one routine(process) control the information". so, this
can never make rece condition happen! 
   But if some day a newb come in then change the code, can you make
sure the criteria again? so we need compiler to enforce the criteria!

*** lexical confinement
#+BEGIN_SRC
package main

import "fmt"

func main() {
	chanOwner := func() <-chan int {
		results := make(chan int, 5)
		go func() {
			defer close(results)
			for i := 0; i <= 5; i++ {
				results <- i
			}
		}()
		return results
	}

	consumer := func(results <-chan int) {
		for result := range results {
			fmt.Printf("Received: %d\n", result)
		}
		fmt.Println("Done receive!")
	}

	results := chanOwner()
	consumer(results)
}
#+END_SRC

  in the previous code, "results"" is under chanOwner's lexical
scope. It confines the write aspect of this channel, so other
go routine can not write to it!

  channel is cocurrent safe by itself, now we inspect some no-concurrent
safe data structure.

#+BEGIN_SRC
    printData := func(wg *sync.WaitGroup, data []byte) {
		defer wg.Done()

		var buff bytes.Buffer
		for _, b := range data {
			fmt.Fprintf(&buff, "%c", b)
		}
		fmt.Println(buff.String())
	}

	var wg sync.WaitGroup
	wg.Add(2)
	data := []byte("golang")
	go printData(&wg, data[:3])
	go printData(&wg, data[3:])

	wg.Wait()
#+END_SRC

  in the previous code snippet, "data" is devided into two part,
and each part belongs to a difference routine.
  C(full) = A(part) + B(part);
  whatever you do in a routine has no effect on another.[also, you
can split data into k parts, and k routines deal with each part].

  pros and cons of confinement:
  pros:
  [1]. no need sync primitives, so good perforcement.
  [2]. the code is simpler to understand.

  cons:
  some times it is difficult to establish confinement.
** for--select 
*** send iteration variables to a channel
#+BEGIN_SRC
package main

import (
	"fmt"
	"io/ioutil"
	"strings"
	"time"
)

func main() {
	done := make(chan int)

	bySlice, err := ioutil.ReadFile("./main.go")
	if err != nil {
		panic(err)
	}

	strSlice := strings.Fields(string(bySlice))

	strStream := strStreamGen(strSlice, done)

	i := 0
	for {
		if i > 20 {
			done <- 1
			break
		}

		i++
		fmt.Println(<-strStream)
		time.Sleep(time.Second)
	}
}

func strStreamGen(strSlice []string, done chan int) <-chan string {
	strStream := make(chan string)
	go func() {
		for _, s := range strSlice {
			select {
			case <-done:
				return
			case strStream <- s:
			}
		}
	}()

	return strStream
}
#+END_SRC
    in the privious code snippet, in strStreamGen function, we create a string channel,
then create a new routine, loop over the string slice, put each element on the channel;
this function finally return a only-read channel out; 

    in main routine, we read on this channel; after get n value from the channel, we 
break the channel; then main routine finish; the channel is closed!

*** create goroutine infinitely waiting to be stopped
#+BEGIN_SRC
	done := make(chan int)

	go func() {
		for {
			select {
			case <-done:
				return
			default:
			}

			fmt.Println("Juming and Dancing!")
			time.Sleep(time.Second)
		}
	}()

	time.Sleep(time.Second * time.Duration(10))
	close(done)
#+END_SRC
** deal with goroutine leak
*** how go routine terminate?
**** it complete its work
**** due to an unrecoverable error, it can not be contiune
**** it has been told by others to stop working
*** an example of go routine leak: 
**** leak example:
#+BEGIN_SRC
    doWork := func(strings <-chan string) <-chan interface{} {
		completed := make(chan interface{})
		go func() {
			defer fmt.Println("doWork exited.")
			defer close(completed)
			for s := range strings {
				fmt.Println(s)
			}
		}()

		return completed
	}

	doWork(nil)

	time.Sleep(10 * time.Second)
	fmt.Println("Done")
#+END_SRC
  the main routine sleep 10 seconds, then exit; we can't see
"doWork exited." message print on the screen; the doWork routine is leaked!
as an counter example, we change the code and solving the leaking problem:
#+BEGIN_SRC
   doWork := func(strings <-chan string) <-chan interface{} {
		completed := make(chan interface{})
		go func() {
			defer fmt.Println("doWork exited.")
			defer close(completed)
			for s := range strings {
				fmt.Println(s)
			}
		}()

		return completed
	}

    genStrings := func() <-chan string {
			strings := make(chan string)
			go func() {
				defer close(strings)
				for i := 0; i < 10; i++ {
					strings <- strconv.Itoa(i)
				}
			}()

			return strings
    }

    strs := genStrings()
	doWork(strs)

	time.Sleep(10 * time.Second)
	fmt.Println("Done")
}
#+END_SRC
   This code soving the leaking problem by give doWork a
real channel!
*** use channel pass cancellation signal
 #+BEGIN_SRC
   doWork := func(done <-chan interface{}, strings <-chan string) <-chan interface{} {
		completed := make(chan interface{})
		go func() {
			defer fmt.Println("doWork exited.")
			defer close(completed)

			for {
				select {
				case s := <-strings:
					fmt.Println(s)
				case <-done:
					return
				}
			}
		}()

		return completed
	}

	done := make(chan interface{})
	terminated := doWork(done, nil)

	go func() {
		time.Sleep(1 * time.Second)
		fmt.Println("Canceling doWork goroutines...")
		close(done)
	}()

	<-terminated
	fmt.Println("Done")
 #+END_SRC

   in main routine we spawn a new routine, which close "done" channel after one second,
then doWork routine's "for-select" get this message, the doWork routine exit and close 
"completed" channel; main "<-terminated" wait on this closed channel and return. look!
no routine leak and deadlock happen!

CONVENTION: If a gorutine is responsible for creating a goroitine, it is also responsible
for ensure it can be stop the goroutine.
** or-channel
   or-channel is used to combine one or more done channels into
a single done channel, if any one of these channels is closed, then
the composed one will be closed.
   snippet code of or-channel:

#+BEGIN_SRC
    var or func(channels ...<-chan interface{}) <-chan interface{}

	or = func(channels ...<-chan interface{}) <-chan interface{} {
		switch len(channels) {
		case 0:
			return nil
		case 1:
			return channels[0]
		}

		orDone := make(chan interface{})
		go func() {
			defer close(orDone)

			switch len(channels) {
			case 2:
				select {
				case <-channels[0]:
				case <-channels[1]:
				}
			default:
				select {
				case <-channels[0]:
				case <-channels[1]:
				case <-channels[2]:
				case <-or(append(channels[3:], orDone)...):
				}
			}
		}()

		return orDone
	}
#+END_SRC

or-channel use case:
#+BEGIN_SRC
	sig := func(after time.Duration) <-chan interface{} {
		c := make(chan interface{})
		go func() {
			defer close(c)
			time.Sleep(after)
		}()

		return c
	}

	start := time.Now()
	<-or(
		sig(2*time.Hour),
		sig(5*time.Minute),
		sig(1*time.Second),
		sig(1*time.Hour),
		sig(1*time.Minute),
	)
	fmt.Printf("done after %v\n", time.Since(start))
#+END_SRC
    after one second, the process will terminated!

	empty select:
#+BEGIN_SRC
package main

import (
	"fmt"
	"sync"
)

func main() {
	var wg sync.WaitGroup

	wg.Add(1)
	go func() {
		defer wg.Done()
		select {}
		fmt.Println("After select")
	}()

	wg.Wait()
}
#+END_SRC
    when we running the code, get this CLI output:
	[[file:./graph/empty_select.png]]
	we know dead lock happen, but when we commented the empty select code line:
    //select {}
	we get this:
	[[file:./graph/empty_select_cmt.png]]
	so, the empty select cause the dead lock!!!
** error handling in concurrent programming
     what can you do when errors occur in goroutine? let's see a little
   silly example:
#+BEGIN_SRC
package main

import (
	"fmt"
	"net/http"
)

func main() {
	checkStatus := func(done <-chan interface{}, urls ...string) <-chan *http.Response {
		responses := make(chan *http.Response)
		go func() {
			defer close(responses)
			for _, url := range urls {
				resp, err := http.Get(url)
				if err != nil {
					fmt.Println(err)  //:) only print the error in go routine, watch me!!!!
					continue
				}

				select {
				case <-done:
					return
				case responses <- resp:
				}
			}
		}()
		return responses
	}

	done := make(chan interface{})
	defer close(done)

	urls := []string{"https://www.baidu.com", "https://badhost"}
	for response := range checkStatus(done, urls...) {
		fmt.Printf("Response: %v\n", response.Status)
	}
}
#+END_SRC
    The previous code get sites responses, if success, move it into the result channel;
if failed, just print the error message in the work goroutine and continue work!!
    So the father routine(here is main routine) know nothing about the error(s) in his
child routine, thought he has the full context of the logic, he can do nothing with these
error(s).What a big tragedy!

    smater_example:
#+BEGIN_SRC
package main

import (
	"fmt"
	"net/http"
)

func main() {
	type Result struct {
		Error    error
		Response *http.Response
	}

	checkStatus := func(done <-chan interface{}, urls ...string) <-chan Result {
		results := make(chan Result)
		go func() {
			defer close(responses)
			for _, url := range urls {
				var result Result
				resp, err := http.Get(url)
				result = Result{err, resp}

				select {
				case <-done:
					return
				case results <- result:
				}
			}
		}()
		return results
	}

	done := make(chan interface{})
	defer close(done)

	urls := []string{"https://www.baidu.com", "https://badhost"}
	for result := range checkStatus(done, urls...) {
		if result.Error != nil {
			fmt.Printf("error: %v\n", result.Error)
		}
		fmt.Printf("Response: %v\n", results.Response.Status)
	}
}
#+END_SRC

    in previous smater example, we compose error result and normal result in
a struct called Result, and return a channel which type is this kind of struct!
now the main routine know all the result of his child routines! he can do 
what he want to deal with this messages!
** pipeline pattern
*** function pipeline in golang
**** batching processing
#+BEGIN_SRC go
package main

import "fmt"

func main() {
	multiply := func(values []int, multiplier int) []int {
		multipliedValues := make([]int, len(values))
		for i, v := range values {
			multipliedValues[i] = v * multiplier
		}
		return multipliedValues
	}

	add := func(values []int, adder int) []int {
		addedValues := make([]int, len(values))
		for i, v := range values {
			addedValues[i] = v + adder
		}
		return addedValues
	}

	ints := []int{1, 2, 3, 4}
	for _, v := range add(multiply(ints, 2), 1) {
		fmt.Println(v)
	}
}
#+END_SRC
    the privious code simulate a batching process scene. every
function eat a batch of data and pull out the same kind batch of
data. It is something just like functional programming:
#+BEGIN_SRC lisp
(defun multi-lst (lst n)
  (mapcar #'(lambda (x) (* x n))
		  lst))

(defun add-lst (lst n)
  (mapcar #'(lambda (x) (+ x n))
		  lst))

(add-lst (multi-lst '(1 2 3 4) 2) 1)
#+END_SRC
    you can see how nature functional programming(here common lisp)
implement this kind of batching pipeline process.
**** stream processing
#+BEGIN_SRC go
    multiply := func(value, multiplier int) int {
		return value * multiplier
	}

	add := func(value, adder int) int {
		return value + adder
	}

	ints := []int{1, 2, 3, 4}
	for _, v := range ints {
		fmt.Println(add(multiply(v, 2), 1))
	}
#+END_SRC
    the cons of the privious code is obvious: we have
to instantialize a new pipe line in each iteration.
*** use channel construct pipeline
*** channel processing[manifest previous example]
#+BEGIN_SRC go
package main

import "fmt"

func main() {
	generator := func(done <-chan interface{}, integers ...int) <-chan int {
		intStream := make(chan int)
		go func() {
			defer close(intStream)
			for _, i := range integers {
				select {
				case <-done:
					return
				case intStream <- i:
				}
			}
		}()
		return intStream
	}

	multiply := func(done <-chan interface{}, intStream <-chan int, multiplier int) <-chan int {
		multipliedStream := make(chan int)
		go func() {
			defer close(multipliedStream)
			for i := range intStream {
				select {
				case <-done:
					return
				case multipliedStream <- i * multiplier:
				}
			}
		}()
		return multipliedStream
	}

	add := func(done <-chan interface{}, intStream <-chan int, adder int) <-chan int {
		addedStream := make(chan int)
		go func() {
			defer close(addedStream)
			for i := range intStream {
				select {
				case <-done:
					return
				case addedStream <- i + adder:
				}
			}
		}()
		return addedStream
	}

	done := make(chan interface{})
	defer close(done)

	intStream := generator(done, 1, 2, 3, 4)
	pipeline := multiply(done, add(done, multiply(done, intStream, 2), 1), 2)

	for v := range pipeline {
		fmt.Println(v)
	}
}
#+END_SRC

what the generator has done? it converts a discrete set of values into a stream
of data on a channel.(This type of function is called generator)

the biggest difference in channel pipeline and function pipe line: use the channel
primitives, each stages of the pipeline is excuting cocurrently.
*** some handy generator
#+BEGIN_SRC
package main

import "fmt"

func main() {
	repeat := func(done <-chan interface{}, values ...interface{}) <-chan interface{} {
		valueStream := make(chan interface{})
		go func() {
			defer close(valueStream)
			for {
				for _, v := range values {
					select {
					case <-done:
						return
					case valueStream <- v:
					}
				}
			}
		}()
		return valueStream
	}

	take := func(done <-chan interface{}, valueStream <-chan interface{}, num int) <-chan interface{} {
		takeStream := make(chan interface{})
		go func() {
			defer close(takeStream)
			for i := 0; i < num; i++ {
				select {
				case <-done:
					return
				case takeStream <- <-valueStream:
				}
			}
		}()
		return takeStream
	}

	done := make(chan interface{})
	defer close(done)

	for num := range take(done, repeat(done, 10), 10) {
		fmt.Printf("%v ", num)
	}
}
#+END_SRC
    in the privious code, "repeat" will repeat the value you pass to it infinitely until you tell it to stop;
    "take" take the first num items off of its incoming stream if it is not closed so early!

	let's see a new kind of repeat:
#+BEGIN_SRC
repeatFn := func(done <-chan interface{}, fn func() interface{}) <-chan interface{} {
		valueStream := make(chan interface{})
		go func() {
			defer close(valueStream)
			for {
				select {
				case <-done:
					return
				case valueStream <- fn():
				}
			}
		}()
		return valueStream
	}
#+END_SRC
    "repeatFn" infinitely move the result of fn to the channel, we can use it like this:
#+BEGIN_SRC
	done := make(chan interface{})
	defer close(done)

	rand := func() interface{} {
		return rand.Int()
	}

	for num := range take(done, repeatFn(done, rand), 10) {
		fmt.Println(num)
	}
#+END_SRC
    an infinite channel of random integers.
**** interface{} and type assertion stage
	 in the previous example, we let each stage eat interface{} and pull out
 interface{}, how we want a stage deal with specific type?
 #+BEGIN_SRC
     toString := func(done <-chan interface{}, valueStream <-chan interface{}) <-chan string {
		 stringStream := make(chan string)
		 go func() {
			 defer close(stringStream)
			 for v := range valueStream {
				 select {
				 case <-done:
					 return
				 case stringStream <- v.(string):
				 }
			 }
		 }()
		 return stringStream
	 }
 #+END_SRC
 #+BEGIN_SRC
     var message string
	 for token := range toString(done, take(done, repeat(done, "a", "b"), 10)) {
		 message += token
	 }

	 fmt.Printf("message: %s...\n", message)
 #+END_SRC
** Fan-out, Fan-in
the the privious pipeline pattern has a big problem, if one of the middle stage
in the pipeline is computationally expensive, it will eclipse the performance 
overhead.

** or-done-channel
when goroutine are reading from a channel, suddenly the routine is canceled, but
how do you insure the channel which is being reading is closed too? you can implement
it like this:

#+BEGIN_SRC go
orDone := func(done, c <-chan interface{}) <-chan interface{} {
		valStream := make(chan interface{})
		go func() {
			defer close(valStream)
			for {
				select {
				case <-done:
					return
				case v, ok := <-c:
					if ok == false {
						return
					}
					select {
					case valStream <- v:
					case <-done:
					}
				}
			}
		}()
		return valStream
	}

	done := make(chan interface{})
	defer close(done)

	getoutchan := func() <-chan interface{} {
		outchan := make(chan interface{})
		go func() {
			for i := 0; i < 100; i++ {
				outchan <- i
			}
		}()
		return outchan
	}

	outerchan := getoutchan()

	ordonechan := orDone(done, outerchan)
	for v := range ordonechan {
		fmt.Println(v)
		time.Sleep(time.Second)
	}
#+END_SRC

in the privious code snippet, outerchan is getting by call getoutchan(); it can be treated
like a channel from other component of this program; we use *orDone* fileter out a new
channel *ordonechan*, we the *close(done)* is called, it can be insure that this *ordonechan*
will be closed!

** tee-channel
in linux/unix system, you can use the *tee* command to sperate out
a new data stream to file:
#+BEGIN_SRC sh
ls -l | tee test.txt | wc -l
#+END_SRC
the result of "ls -l" will dump to test.txt file, and a same duplication
will be regarded as the input of "wc -l"

#+BEGIN_SRC go
tee := func(done <-chan interface{}, in <-chan interface{}) (_, _ <-chan interface{}) {
		out1 := make(chan interface{})
		out2 := make(chan interface{})

		go func() {
			defer close(out1)
			defer close(out2)
			for val := range orDone(done, in) {
				var out1, out2 = out1, out2
				for i := 0; i < 2; i++ {
					select {
					case <-done:
					case out1 <- val:
						out1 = nil
					case out2 <- val:
						out2 = nil
					}
				}
			}
		}()
		return out1, out2
	}

	done := make(chan interface{})
	defer close(done)

	getoutchan := func() <-chan interface{} {
		outchan := make(chan interface{})
		go func() {
			for i := 0; i < 100; i++ {
				outchan <- i
			}
		}()
		return outchan
	}

	outerchan := getoutchan()

	stream1, stream2 := tee(done, outerchan)
	for v := range orDone(done, stream1) {
		fmt.Printf("stream1: %v, stream2: %v\n", v, <-stream2)
		time.Sleep(time.Second)
	}
#+END_SRC

** bridge-channel
the difference bettween "a sequece of channels" and "a slice of channels":
 
|----chan1---- |-----chan2------- |------chan3------- |
sequece of channels is just "channel of channels", elements in inner channel(such chan1)
have order, channel in out also have an order(chan1 > chan2 > chan3); let's see how to
generate such a "sequece of channels":

#+BEGIN_SRC go
genChanSeq := func() <-chan <-chan interface{} {
		rand.Seed(time.Now().UnixNano())
		seqChan := make(chan (<-chan interface{}))
		go func() {
			for {
				//generate inner channel
				rndNum := 1 + rand.Intn(10)
				innerChan := make(chan interface{}, rndNum)

				for i := 0; i < rndNum; i++ {
					innerChan <- "str" + strconv.Itoa(rndNum)
				}

				seqChan <- innerChan
				close(innerChan)

				time.Sleep(time.Second)
			}
		}()

		return seqChan
	}
#+END_SRC
in the previous code snippet, *genChanSeq* will generate a sequence of channels,
every inner channel in this "sequence" is a buffered channel contains random strings;

we can get all the elements out like this:
#+BEGIN_SRC go
	chanseq := genChanSeq()
	//do not use briage channel
	for innerChan := range chanseq {
		for v := range innerChan {
			fmt.Println(v)
		}
	}
#+END_SRC

the code is some kind of verbose, we can use bridge-channel to merge them into one channel:
#+BEGIN_SRC go
	bridge := func(done <-chan interface{}, chanStream <-chan <-chan interface{}) <-chan interface{} {
		valStream := make(chan interface{})
		go func() {
			defer close(valStream)
			for {
				//get a inner channel
				var stream <-chan interface{}
				select {
				case maybeStream, ok := <-chanStream:
					if ok == false {
						return
					}
					stream = maybeStream
				case <-done:
					return
				}

				//interate in the inner channel
				for val := range orDone(done, stream) {
					select {
					case valStream <- val:
					case <-done:
					}
				}
			}
		}()
		return valStream
	}
#+END_SRC
after bridge the sequence of channels, we can get a final value stream:
#+BEGIN_SRC go
	done := make(chan interface{})
	defer close(done)

	seq := bridge(done, genChanSeq())
	for v := range seq {
		fmt.Println(v)
	}
#+END_SRC
now, the logic in the code is better clear;

** use channel just like a queue
A Critia: Queuing Will Almost Never Speed Up The Total Runtime Of Your Program; 
It Will Only Allow The Program To Behave Differently.

** the context package
In the previous patterns, we use "done" channel to cancel all blocking concurrent
oprations. but the the "done" tell nothing about why the cancelling happening! So
it would be useful if add some extra information with the done channel.

done + some extra info ==> the context package is comming out:
#+BEGIN_SRC go
var Canceled = errors.New("context canceled")
var DeadlineExceeded error = deadlineExceededError{}
func WithCancel(parent Context) (ctx Context, cancel CancelFunc)
func WithDeadline(parent Context, d time.Time) (Context, CancelFunc)
func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc)
type CancelFunc func()
type Context interface{ ... }
func Background() Context
func TODO() Context
func WithValue(parent Context, key, val interface{}) Context
#+END_SRC

now let's see the content of *Context* interface:
#+BEGIN_SRC go
type Context interface {
	// Deadline returns the time when work done on behalf of this context
	// should be canceled. Deadline returns ok==false when no deadline is
	// set. Successive calls to Deadline return the same results.
	Deadline() (deadline time.Time, ok bool)

	// Done returns a channel that's closed when work done on behalf of this
	// context should be canceled. Done may return nil if this context can
	// never be canceled. Successive calls to Done return the same value.
	//
	// WithCancel arranges for Done to be closed when cancel is called;
	// WithDeadline arranges for Done to be closed when the deadline
	// expires; WithTimeout arranges for Done to be closed when the timeout
	// elapses.
	//
	// Done is provided for use in select statements:
	//
	//  // Stream generates values with DoSomething and sends them to out
	//  // until DoSomething returns an error or ctx.Done is closed.
	//  func Stream(ctx context.Context, out chan<- Value) error {
	//  	for {
	//  		v, err := DoSomething(ctx)
	//  		if err != nil {
	//  			return err
	//  		}
	//  		select {
	//  		case <-ctx.Done():
	//  			return ctx.Err()
	//  		case out <- v:
	//  		}
	//  	}
	//  }
	//
	// See https://blog.golang.org/pipelines for more examples of how to use
	// a Done channel for cancellation.
	Done() <-chan struct{}

	// If Done is not yet closed, Err returns nil.
	// If Done is closed, Err returns a non-nil error explaining why:
	// Canceled if the context was canceled
	// or DeadlineExceeded if the context's deadline passed.
	// After Err returns a non-nil error, successive calls to Err return the same error.
	Err() error

	// Value returns the value associated with this context for key, or nil
	// if no value is associated with key. Successive calls to Value with
	// the same key returns the same result.
	//
	// Use context values only for request-scoped data that transits
	// processes and API boundaries, not for passing optional parameters to
	// functions.
	//
	// A key identifies a specific value in a Context. Functions that wish
	// to store values in Context typically allocate a key in a global
	// variable then use that key as the argument to context.WithValue and
	// Context.Value. A key can be any type that supports equality;
	// packages should define keys as an unexported type to avoid
	// collisions.
	//
	// Packages that define a Context key should provide type-safe accessors
	// for the values stored using that key:
	//
	// 	// Package user defines a User type that's stored in Contexts.
	// 	package user
	//
	// 	import "context"
	//
	// 	// User is the type of value stored in the Contexts.
	// 	type User struct {...}
	//
	// 	// key is an unexported type for keys defined in this package.
	// 	// This prevents collisions with keys defined in other packages.
	// 	type key int
	//
	// 	// userKey is the key for user.User values in Contexts. It is
	// 	// unexported; clients use user.NewContext and user.FromContext
	// 	// instead of using this key directly.
	// 	var userKey key
	//
	// 	// NewContext returns a new Context that carries value u.
	// 	func NewContext(ctx context.Context, u *User) context.Context {
	// 		return context.WithValue(ctx, userKey, u)
	// 	}
	//
	// 	// FromContext returns the User value stored in ctx, if any.
	// 	func FromContext(ctx context.Context) (*User, bool) {
	// 		u, ok := ctx.Value(userKey).(*User)
	// 		return u, ok
	// 	}
	Value(key interface{}) interface{}
}
#+END_SRC

*** use context as API for cancelling branches of routines call-graph:
what is "routine cancel":
1. A goroutine’s parent may want to cancel it.
2. A goroutine may want to cancel its children.
3. Any blocking operations within a goroutine need to be preemptable so that it
may be canceled.

**** use the done channel pattern to cancelling routines:
#+BEGIN_SRC go
package main

import (
	"fmt"
	"sync"
	"time"
)

func main() {
	var wg sync.WaitGroup
	done := make(chan interface{})
	defer close(done)
	wg.Add(1)
	go func() {
		defer wg.Done()
		if err := printGreeting(done); err != nil {
			fmt.Printf("%v", err)
			return
		}
	}()
	wg.Add(1)
	go func() {
		defer wg.Done()
		if err := printFarewell(done); err != nil {
			fmt.Printf("%v", err)
			return
		}
	}()
	wg.Wait()
}

func printGreeting(done <-chan interface{}) error {
	greeting, err := genGreeting(done)
	if err != nil {
		return err
	}
	fmt.Printf("%s world!\n", greeting)
	return nil
}

func printFarewell(done <-chan interface{}) error {
	farewell, err := genFarewell(done)
	if err != nil {
		return err
	}
	fmt.Printf("%s world!\n", farewell)
	return nil
}

func genGreeting(done <-chan interface{}) (string, error) {
	switch locale, err := locale(done); {
	case err != nil:
		return "", err
	case locale == "EN/US":
		return "hello", nil
	}
	return "", fmt.Errorf("unsupported locale")
}

func genFarewell(done <-chan interface{}) (string, error) {
	switch locale, err := locale(done); {
	case err != nil:
		return "", err
	case locale == "EN/US":
		return "goodbye", nil
	}
	return "", fmt.Errorf("unsupported locale")
}

func locale(done <-chan interface{}) (string, error) {
	select {
	case <-done:
		return "", fmt.Errorf("canceled")
	case <-time.After(1 * time.Minute):
	}
	return "EN/US", nil
}
#+END_SRC
**** use context.Context manage routines:
#+BEGIN_SRC go
package main

import (
	"context"
	"fmt"
	"sync"
	"time"
)

func main() {
	var wg sync.WaitGroup
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	wg.Add(1)
	go func() {
		defer wg.Done()

		if err := printGreeting(ctx); err != nil {
			fmt.Printf("can not print greeting: %v\n", err)
			cancel()
		}
	}()

	wg.Add(1)
	go func() {
		defer wg.Done()

		if err := printFarewell(ctx); err != nil {
			fmt.Printf("can not print farewell: %v\n", err)
			cancel()
		}
	}()

	wg.Wait()
}

func printGreeting(ctx context.Context) error {
	greeting, err := genGreeting(ctx)
	if err != nil {
		return err
	}

	fmt.Printf("%s world!\n", greeting)
	return nil
}

func printFarewell(ctx context.Context) error {
	farewell, err := genFarewell(ctx)
	if err != nil {
		return err
	}

	fmt.Printf("%s world!\n", farewell)
	return nil
}

func genGreeting(ctx context.Context) (string, error) {
	ctx, cancel := context.WithTimeout(ctx, 1*time.Second)  //key point
	defer cancel()

	switch locale, err := locale(ctx); {
	case err != nil:
		return "", err
	case locale == "EN/US":
		return "hello", nil
	}

	return "", fmt.Errorf("unsupported locale")
}

func genFarewell(ctx context.Context) (string, error) {
	switch locale, err := locale(ctx); {
	case err != nil:
		return "", err
	case locale == "EN/US":
		return "goodbye", nil
	}

	return "", fmt.Errorf("unsupported locale")
}

func locale(ctx context.Context) (string, error) {
	select {
	case <-ctx.Done():
		return "", ctx.Err()
	case <-time.After(1 * time.Minute):
	}

	return "EN/US", nil
}
#+END_SRC
*** use context as a data-bag for transporting request-scoped data through call-graph:
* Concurrency at scale
