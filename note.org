* Git
** git create and merge a branch
*** create a branch
git checkout -b iss53
*** merge a branch into a master
 git checkout master
 git pull origin master
 git merge test
 git push origin master
*** delete a brach local and remote
 $ git push --delete <remote_name> <branch_name>
 $ git branch -d <branch_name>
 Note that in most cases the remote name is origin.

 Delete Local Branch
 To delete the local branch use one of the following:
 $ git branch -d branch_name
 $ git branch -D branch_name
** git add(delete) tags locally and remotelly
*** delete tags locally and remotelly
 git push --delete origin tagname
 git tag --delete tagname
*** add and push tag remotelly
 git tag <your tag name>
 git tag <your tag name> -a        This will add some description to your new added tag
   
 git push origin --tags            [push all your tags to remote]
 git push origin <your tag name>   [push a single tag to remote]
** git proxy set and unset
   git config --global https.proxy http://127.0.0.1:1080
   git config --global https.proxy sock5://127.0.0.1:1080
   git config --global https.proxy https://127.0.0.1:1080
   git config --global http.proxy sock5://127.0.0.1:1080

   git config --global --unset http.proxy
   git config --global --unset https.proxy
   npm config delete prox
** using git tags:
*** the difference bettween branches and tags:
   branches are mutable references
   tags are immutable references
   [[https://stackoverflow.com/questions/18216991/create-a-tag-in-a-github-repository][git tags]]
** deal with merge conflict
*** https://stackoverflow.com/questions/161813/how-to-resolve-merge-conflicts-in-git 
** deal with remote repo
*** [[https://git-scm.com/book/en/v2/Git-Basics-Working-with-Remotes][git deal with remote repo]]
** using git under emacs -- magit
  
* Clojure
** DONE build, running and the REPL
** DONE install jvm and Leiningen
   choco install jvm1.8
   [[http://leiningen.org][leiningen site]]
   [[https://djpowell.github.io/leiningen-win-installer/][install leiningen on windows mechine]]
   after leiningen installed, enter
   *$lein new app hello-world*
   to create a clojure project

   then cd into hello-world, enter
   *$lein deps*
   to load dependences file form centrol

** DONE let emacs dance with clojure
Table 1: Clojure Buffer Key Bindings
| Keys	        | Description                                                                        |
|-----------------+------------------------------------------------------------------------------------|
| C-c M-n M-n	 | Switch to namespace of current buffer.                                             |
| C-x C-e	     | Evaluate expression immediately preceding point.                                   |
| C-c C-k	     | Compile current buffer.                                                            |
| C-c C-d C-d	 | Display documentation for symbol under point.                                      |
| M-. and M-,	 | Navigate to source code for symbol under point and return to your original buffer. |
| C-c C-d C-a	 | Apropros search; find arbitrary text across function names and documentation.      |

Table 2: CIDER Buffer Key Bindings
| Keys	       | Description                     |
|----------------+---------------------------------|
| C-↑, C-↓	 | Cycle through REPL history.     |
| C-enter	    | Close parentheses and evaluate. |


| Keys             | Description                                                              |
|------------------+--------------------------------------------------------------------------|
| M-x paredit-mode | Toggle paredit mode.                                                     |
| M-(              | Surround expression after point in parentheses (paredit-wrap-round).     |
| C-→             | Slurp; move closing parenthesis to the right to include next expression. |
| C-←             | Barf; move closing parenthesis to the left to exclude last expression.   |
| C-M-f/C-M-b      | Move to the opening/closing parenthesis.                                 |

** DONE Clojure basic data structure
All of Clojures data structures are immutable, meaning you cant change them in place.
*** Numbers in Clojure
*** Strings in Clojure
*** Maps in Clojure
*** Keywords in Clojure
*** Vectors in Clojure
*** Lists in Clojure
*** Function:
**** function arity overloading

#+BEGIN_SRC elisp 
(defn multi-arity
 ;; 3-arity arguments and body
 ([first-arg second-arg third-arg]
   (str first-arg second-arg third-arg))
 ([first-arg second-arg]
   (str first-arg second-arg))
 ([first-arg]
   (str first-arg)))
#+END_SRC

Use arity overloading provide default values for arguments:
#+BEGIN_SRC elisp
(defn x-chop
   "Describe the kind of chop you're inflicting on someone"
   ([name chop-type]
      (str "I " chop-type "chop " name "! Take that!"))
   ([name]
      (x-chop name "karate")))
#+END_SRC

Varible arguments:

#+BEGIN_SRC elisp
(defn codger-communication
   [whippersnapper]
   (str "Get off my lawn, " whippersnapper "!!!"))

(defn codger
   [& whippersnappers]
   (map codger-communication whippersnappers))
#+END_SRC

running it:

(codger "Billy" "Anne-Marie" "The Incredible Bulk")
("Get off my lawn, Billy!!!"
 "Get off my lawn, Anne-Marie!!!"
 "Get off my lawn, The Incredible Bulk!!!")

**** function destruction

** DONE Clojure core functions
** TODO functional programming in Clojure
side effects are potentially harmful. because they introduce uncertainty about what the names in
your code are reffering to.

Functions with size effects, on the other hand, place more of a burden on your mind grapes.
Not only these functions are rot, but those components use these functions. They are infected by.

How can you progamming without size effect? This is really a big suege!! Really BIG!!
*** Living with (Clojure) Immutable Data Structure
Clojure use recursion work around the side effect:
#+BEGIN_SRC Clojure
(defn my-sum
             ([vals] (my-sum vals 0))
             ([vals accumulating-total]
                 (if (empty? vals)
                   accumulating-total
                   (my-sum (rest vals) (+ (first vals) accumulating-total)))))
#+END_SRC

Clojure do not use Attribute Mutation commonly used in some OO programming languange.
Such as C++, java. Alternativly use function composition to implement the Mutation effects!

example:
#+BEGIN_SRC 
(defn clean
             [text]
             (s/replace (s/trim text) #"lol" "LOL"))
#+END_SRC
it is so simple to write programe with function composition, just pass return value of one function
to another function. Compare with OO languanges, the classes in OO languanges is to protect date, 
they just want to fight against unwanted modification of private data. Just as they protect the data,
the functions deal with the data are also been protected! This is often unnessensery.

let's seen the two powerful weapons of Clojure:
1) decoupling functions and data, let them free from each other!
2) programming to a small set of abstractions, this will grow into more
   reusable, composable code!

*** Cool Things to do with Pure functions
Data can be derived into new data in Clojure, but what about function? Can you Drive new functions from exist
pure functions? Let's try it!

**** Let's refresh the memory of "partial"
(defn hundred-times (partial * 100))
(hundred-times 100)

**** Use comp to composite functions
#+BEGIN_SRC 
(def character
     {:name "Smooches McCutes"
      :attributes {:intelligence 10
                   :strength 4
                   :dexterity 5}})


(def c-int (comp :intelligence :attributes))
(def c-str (comp :strength :attributes))
(def c-dex (comp :dexterity :attributes))

(c-int character)
(c-str character)
(c-dex character)
#+END_SRC

* Emacs
** Seach Text in Directory
** Emacs multi-windows mode and windows management
    [[http://ergoemacs.org/misc/emacs_one_max_window_vs_multi_smaller_window.html][kick this link]]
** Using register in emacs
    | -------------- | ---------------------------------- | ------------------ | -------------------------------------------------------------  |
    | Type           | How to save                        | How to use         | Other useful command                                           |
    |----------------+------------------------------------+--------------------+----------------------------------------------------------------|
    | Position       | C-x r <SPC> r                      | jump: C-x r j r    |                                                                |
    | Text           | C-x r s r                          | insert: C-x r i r  | m-x append-toregister <RET> r; m-x prepend-to-register <RET> r |
    | Rectangle      | C-x r r r                          | insert: C-x r i r  |                                                                |
    | Window Config  | C-x r w r                          | restore: C-x r j r | save all frame's window: C-x r f r                             |
    | Number         | C-u number C-x r n r               | insert: C-x r i r  | increment: C-x r + r                                           |
    | File           | (set-register ?z '(fine . name))   | jump: C-x r j r    |                                                                |
** Emacs return to privious position
   [Ctrl + Space] [Ctrl + Space] save current position into the mark ring
   do some editing
   [Ctrl + u] [Ctrl + Space] move back to privous position
* Emacs Org
** DONE Deal with table
*** Preparing tables for export
	
** Emacs org mode for GTD(Get Things Done)
** Org super agenda
* Golang
** Viper(小蛇)
*** What is Viper and What you can play with viper?
Viper is a complete configuration solution for Go applications including 12-Factor apps. It is designed to work within an application, and can handle all types of configuration needs and formats. It supports:

1. setting defaults
2. reading from JSON, TOML, YAML, HCL, envfile and Java properties config files
3. live watching and re-reading of config files (optional)
4. reading from environment variables
5. reading from remote config systems (etcd or Consul), and watching changes
6. reading from command line flags
7. reading from buffer
8. setting explicit values

Viper can be thought of as a registry for all of your applications configuration needs.

** OpenTracing(Golang)
"Ditribute tracing" is particularly well-suited for debuging and nonitoring morden distribute arch.
*** Concept and Terminology(reading OpenTracing Semantic Specification)
**** Data Model
Trace is defined implicitly by spans

Example Trace:

         [Span A]  ←←←(the root span)
            |
     +------+------+
     |             |
 [Span B]      [Span C] ←←←(Span C is a `ChildOf` Span A)   [ChildOf]:
     |             |
 [Span D]      +---+-------+
               |           |
           [Span E]    [Span F] >>> [Span G] >>> [Span H]      [FollowFrom]:
                                       ↑
                                       ↑
                                       ↑
                         (Span G `FollowsFrom` Span F)


We just visualize This trace in a time axis:

––|–––––––|–––––––|–––––––|–––––––|–––––––|–––––––|–––––––|–> time
 [Span A···················································]
   [Span B··············································]
      [Span D··········································]
    [Span C········································]
         [Span E·······]        [Span F··] [Span G··] [Span H··]


Every span contains the following state:
[] op name
[] start timestamp
[] finish timestamp
[] span tags, key/value pairs
[] span logs, key/value pairs with a timestamp
[] a SpanContext
[] References to other spans

each SpanContext contains:
[] opentracing-implementation-dependent state(trace and span ids)
[] baggage items(key/value pairs)

**** OpenTracing API
Tracer interface:
Tracer interface create Spans and understands how to Inject(serialize) and
Extract(deserialize) them across process boundaries.
Capabilities:
[] Start a new Span
[] Inject a SpanContext into a carrier
[] Extract a SpanContext from a carrier

Span interface:
Capabiiities:
[] Retrive the Spans SpanContext
[] Overwrite the operation name
[] Finish the Span
[] Set a Span Tag
[] Log structed data
[] Set/Get baggage item

**** OpenTracing API for GoLang(yurushkuro opentracing tutorial)
***** install jaeger in a docker image
docker run \
  --rm \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 16686:16686 \
  jaegertracing/all-in-one:1.7 \
  --log-level=debug

if jaeger is not install, this command will install it into docker

Once the jeager backend start, UI will be accessible at [[http://localhost:16696][http://localhost:16686]]

***** Hello Open-Tracing
****** no-op tracing
[[file:~/PlayGround/OpenTracing/hello_tracing_noop.go][noop_tracing_example]] opentracing.GlobalTracer() returns a no-op tracer by default.

****** Initialize a real tracer with jeager 
[[~/PlayGround/OpenTracing/hello_tracing_jeager.go]]

run it then you can see the trace in jeager UI

****** Annotate the trace with tags and logs
name you span, when we use tags vs logs
[[~/PlayGround/OpenTracing/hello_tracing_taglog.go]]
Now, run and look at the jeager UI again, absolutly you can see the tags and logs

***** Context and Tracing Functions
we emphis on the following topics:
[] Tracing individual functions
[[~/PlayGround/OpenTracing/hello_tracing_indiv_funcs.go]]
[] Combine multiple spans into a single trace
[[~/PlayGround/OpenTracing/hello_tracing_merge_spans.go]]
[] propagate the in-process context
In the privious example, we pass something(span) as function parameter to link the individual spans 
together, but this will pollute our application code by introducing tracing code. So we use context.Context
to deal with it!
[[~/PlayGround/OpenTracing/hello_tracing_context_spans.go]]
***** A client-server tracing deamon
client: [[~/PlayGround/OpenTracing/BinaryOpenTracing/client.go]]
      server: [[~/PlayGround/OpenTracing/BinaryOpenTracing/server.go]]

** Golang Archive Package
*** archive/tar access to tar archive

** Golang zip(deflate compress) a string or a byte array
   archive/zip is used to zip and unzip file, it is so difficult
   to zip/unzip string or byte slice. so it is time to use compress/flate
   instead. The follow is the code i use:
#+BEGIN_SRC 
   package main

import (
	"bytes"
	"compress/flate"
	"fmt"
	"io"
	"os"
)

func main() {
	buf := new(bytes.Buffer)
	flateWriter, err := flate.NewWriter(buf, flate.BestCompression)
	if err != nil {
		panic(err)
	}
	defer flateWriter.Close()

	flateWriter.Write([]byte("This is the end of the world!"))
	flateWriter.Flush()
	fmt.Print("After flate compress: %s\n", buf)

	//unpress the flate buffer
	flateReader := flate.NewReader(buf)
	defer flateReader.Close()

	fmt.Println("After unpress: ")
	io.Copy(os.Stdout, flateReader)
}
#+END_SRC
** Golang zlib compress and decompress in networking programming(with java)
*** Java Server
#+BEGIN_SRC java 
import java.net.*;
import java.io.*;
import java.util.*;
import java.util.zip.*;

public class Server
{
	//initialize socket and input stream
	private Socket socket = null;
	private ServerSocket server = null;
	private DataInputStream in = null;

	// constructor with port
	public Server(int port)
	{
		//starts server and waits for a connection
		try
		{
			server = new ServerSocket(port);
			System.out.println("Server started");

			System.out.println("Wait for a client ...");

			socket = server.accept();
			System.out.println("Client accepted");

			// take input from the client socket
			in = new DataInputStream(
			   new BufferedInputStream(socket.getInputStream()));

			// reads message from client 
			try
			{
				byte[] recvData = new byte[1024];
				int cnt = in.read(recvData);
		        byte[] usefulData = Arrays.copyOfRange(recvData, 0, cnt);

				System.out.println(cnt);

				try {
					//decompress the bytes using zlib
					Inflater decompresser = new Inflater();
					System.out.println(cnt);
					decompresser.setInput(usefulData, 0, cnt);
					byte[] decomResult = new byte[1024];
					int decomLen = decompresser.inflate(decomResult);
					System.out.println(decomLen);
					decompresser.end();

					

			   	 	String s = new String(decomResult, 0, decomLen, "UTF-8");
					System.out.println(s);
				} catch (java.util.zip.DataFormatException ex) {
					System.out.println(ex);
				}
			}
			catch(IOException i)
			{
				System.out.println(i);
			}
		
			//close connection
			socket.close();
			in.close();
		}
		catch(IOException i)
		{
			System.out.println(i);
		}
	}

	public static void main(String args[]) {
		Server server = new Server(5000);
	}
}
#+END_SRC
*** Golang Clinet
#+BEGIN_SRC go
package main

import (
	"bytes"
	"compress/zlib"
	"fmt"
	"net"
	"time"
)

func main() {
	serverConn, err := net.Dial("tcp", "localhost:5000")
	if err != nil {
		panic(err)
	}

	var b bytes.Buffer
	w := zlib.NewWriter(&b)
	w.Write([]byte("This is the start of a new life"))
	w.Close()

	n, err := serverConn.Write(b.Bytes())
	if err != nil {
		panic(err)
	}
	fmt.Printf("send %d bytes data to server.\n", n)

	time.Sleep(5 * time.Second)
}
#+END_SRC
  Becareful, when you finish your compress(decompress) action, close your 
  compressor(decompressor) immediately
** Golang concurrency pattern -- context 
   In Go servers, each incoming request is handled in its own goroutine. 
   Request handlers often start additional goroutines to access backends 
   such as databases and RPC services.
 
   The set of goroutines working on a request typically needs access to 
   request-specific values such as the identity of the end user, 
   authorization tokens, and the request's deadline. When a request is 
   canceled or times out, all the goroutines working on that request should 
   exit quickly so the system can reclaim any resources they are using.
   [如何处理一个请求相关的所有goroutines的退出？]

   At Google, we developed a context package that makes it easy to pass 
   request-scoped values, cancelation signals, and deadlines
   [context中所传递的信息] 
   across API boundaries to all the goroutines involved in handling a request.
*** Context interface
#+BEGIN_SRC 
  type Context interface {
    // Done returns a channel that is closed when this Context is canceled
    // or times out.
    Done() <-chan struct{}

    // Err indicates why this context was canceled, after the Done channel
    // is closed.
    Err() error

    // Deadline returns the time when this Context will be canceled, if any.
    Deadline() (deadline time.Time, ok bool)

    // Value returns the value associated with key or nil if none.
    Value(key interface{}) interface{}
   }
#+END_SRC

  Do not store Contexts inside a struct type; instead, pass a Context explicitly to each function that needs it. The Context should be the first parameter, typically named ctx:
#+BEGIN_SRC 
  func DoSomething(ctx context.Context, arg Arg) error {
	// ... use ctx ...
  }
#+END_SRC
*** Create context
**** background context
      ctx, cancel := context.Background()
     This should be only used at a high level(in main or the top level request handler)
**** TDTO context
      ctx, cancel := context.TODO()
      this also create an empty context
*** Derive context 
**** WithValue
     context.WithValue(parent Context, key, val interface{}) (ctx Context, cancel CancelFunc)
     once you get a context with value, any context that derives from this gets this value
**** WithCancel
     context.WithCancel(parent Context) (ctx Context, cancel CancelFunc)
     you can pass around the ctx, but *Never* pass the /cancel/ function
**** WithDeadline
     context.WithDeadline(parent Context, d time.Time) (ctx Context, cancel CancelFunc)
     ctx, cancel := context.WithDeadline(context.Background(), time.Now().Add(2 * time.Second))
*** Accept and use context
** refload refactor
  aaa|bbb|ccc|ddd|2|f1|f2|f3|f4
  aaa|bbb|ccc|ddd|3|f1|f2|f3|d1|d2|d3
** Golang package management 
*** using  go modules
**** Create a new module
go mod init example.com/hello  ---> this will create a new module
the go.mod file only appears in the root of the module
**** Add a dependency to the module

** Use hugo build static site

** Install the latest golang on ubuntu
 sudo add-apt-repository ppa:longsleep/golang-backports
 sudo apt-get update
 sudo apt-get install golang-go
 go version
** Golang module proxy privacy
***   GoCenter is much faster than github?
***  google by default:
   GOPROXY --default--> https://proxy.golang.org,dirrect
   go get
   go build will attempt fecth modules from the Go proxy
***   how you can change it?

***  使用GOPROXY环境变量
    export GO111MODULE=on
    export GOPROXY=https://goproxy
** Golang document
    go doc FUNC OR MODULE NAME
** Prometheus
*** main features
**** a multi-dimensional data model with time series data identified by metric name and key/value pairs
**** PromQL, a flexible query language to leverage this dimensionality
**** no reliance on distributed storage; single server nodes are autonomous
**** time series collection happens via a pull model over HTTP
**** pushing time series is supported via an intermediary gateway
**** targets are discovered via service discovery or static configuration
**** multiple modes of graphing and dashboarding support
*** main components
****    the main Prometheus server which scrapes and stores time series data
****    client libraries for instrumenting application code
****    a push gateway for supporting short-lived jobs
****    special-purpose exporters for services like HAProxy, StatsD, Graphite, etc.
****    an alertmanager to handle alerts
****    various support tools
*** structure of promethus
	[[https://prometheus.io/docs/introduction/overview/][structure link]]
	
*** What we can do with promethus
**** Monitoring Linux host metrics with the Node Exporter
     用Node Exporter监控Linux主机
     [[https://prometheus.io/docs/guides/node-exporter/][Moniter linux node]] 

**** Monitoring Your golang program(Instrumenting a program)
****** chose the client lib for your program
        golang -- golang lib
        c      -- c lib
        ...

** NSQ (and some other distribute message queues)
*** RabbitMQ and Kafka:
	RabbitMQ: message routine function is the killing skill
	[[https://jack-vanlightly.com/blog/2017/12/4/rabbitmq-vs-kafka-part-1-messaging-topologies][Jack Vanlightly, RabbitMQ and Kafka]]
*** Quick Start:
     [[http://tleyden.github.io/blog/2014/11/12/an-example-of-using-nsq-from-go/][golang and nsq, just producer and consumer]]
     [[https://blog.charmes.net/post/first-look-nsq/]]
** Test gidora nsq wrapper:
   
** Debug a golang program
* Essays
** The power of the defauts
 search engine results click
 top search hit ---> 42%
 second search hit ---> 8%
 
 the top hit's attraction:
 there is a strong bias in favor of clicking the top link

 default valuse beyond search:
* One Linux commad One day(common usage)
** tar
*** Creating an uncompressed tar Archive:   
  $tar cvf file.tar *.c
*** Extracting files from tar Archive:
  $tar xvf file.tar
*** gzip compression on the Archive, using option -z:
  $tar cvzf file.tar.gz *.c
*** Extracting a gzip tar Archive using option -xvzf:
  $tar xvzf file.tar.gz
*** Create compressed tar Achive in Linux using -j
  $tar cvfj file.tar.tbz *.c
*** List the contents of the tar file
	$tar tf file.tar.gz
** for
** grep
** use grep to find a string in multi files(files under a directory)
 grep -rnw 'path/to/somewhere' -e 'pattern'
 + -r or -R is recursive
 + -n is line number 
 + -w stands for match the whole word
 + -l just give the file name of matching files
 
 example:
 1. only search through those file which have .c or .h extensions:
	grep --include=\*.{c,h} -rnw 'path/to/somewhere' -e "pattern"

 2. exclude searching all the file ending with .o extension:
	grep --exclude=*.o -rnw 'path/to/somewhere' -e "pattern"

 3. exclude some directories
	grep --exclude-dir={dir1,dir2,*.dst} -rnw 'path/to/somewhere' -e "pattern"
** sed
*** replace a string in multiple files using sed
  sed -i 's/foo/bar/g' *
** regexp
 [[https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285][regexp introduction]]

** set proxy for command line
    $ export http_proxy="http://PROXY_SERVER:PORT"
	$ export https_proxy="https://PROXY_SERVER:PORT"
	$ export ftp_proxy="http://PROXY_SERVER:PORT"

	authentication:
    $ export http_proxy="http://USER:PASSWORD@PROXY_SERVER:PORT"
	$ export https_proxy="https://USER:PASSWORD@PROXY_SERVER:PORT"
	$ export ftp_proxy="http://USER:PASSWORD@PROXY_SERVER:PORT"

	if you can not clone from a git, try https not sock5 proxy
** ubuntu shadowsock install and config
*** server
** change all files under current dir from dos to unix
    find . -type f -exec dos2unix '{}' \;      
** ssh login without passwd (host a ----login----> host b)
*** gen a pair of authenication keys(no pass phrase) on host a:
    ssh-keygen -t rsa
*** create ~/.ssh on host b:
	ssh user@hostb mkdir -p .ssh
*** append the new public key to user@hostb:.ssh/authorized_keys
    cat .ssh/id_rsa.pub || ssh user@hostb 'cat >> .ssh/authorized_keys'
** add user and group
To add a user you must use the sudo command (for an explanation of what that means, see the RootSudo page). Here are the commands:
To add a user. NOTE: do not use the useradd command.

$ sudo adduser <username>

To see the options for adding a user try the man command.
$ man adduser

Here is a useful example of the useradd command. Why use useradd? It gives a few more options for special cases. To add a user, give her a standard home directory in the /home folder and specify the shell she accesses by default do this:

$ sudo useradd username -m -s /bin/bash 
$ sudo passwd username 

Groups

You might also wish to create a new group for your users.
# sudo addgroup <groupname>

To add a new user to a existing group you would do this:
# sudo adduser <username> audio

To add an existing user to an existing group you can use the same command:
# sudo adduser <username> <groupname>

or
# sudo usermod -aG <groupname> <username>
** dpkg -i  
   After using dpkg, running the following command helped me to install the required dependencies:

   sudo apt-get -f install

   In all, your terminal should look like this:
   
   $ sudo dpkg -i package_with_unsatisfied_dependencies.deb
   dpkg: dependency problems prevent ... 
   [additional messages]

   $ sudo apt-get -f install
   [apt messages]
   Setting up [dependency]...
   Setting up package_with_unsatisfied_dependencies...

* VSCode tips and tricks
** Customize: 
*** Crtl + K  Ctrl + T 更改主题
**** 
* Distribute Systems
** fun and profit?
1. 高屋建瓴

   Distributed programming is the art of solving the same problem that you can solve on a single computer using multiple computers.

   Most things are trivial at a small scale - and the same problem becomes much harder once you surpass a certain size, 
   volume or other physically constrained thing. It's easy to lift a piece of chocolate, it's hard to lift a mountain. 
   It's easy to count how many people are in a room, and hard to count how many people are in a country.
   [如何面对规模增长所带来的管理成本]


   Scalability [可扩展]
   is the ability of a system, network, or process, to handle a growing amount of work in a capable manner or its ability to be enlarged to 
   accommodate that growth.
   [可扩展性是一种能力，是一个系统，网络，程序面对不断增长的工作量的能力]

*** Size scalability: 
    adding more nodes should make the system linearly faster; growing the dataset should not increase latency
*** Geographic scalability: 
    it should be possible to use multiple data centers to reduce the time it takes to respond to user queries, 
    while dealing with cross-data center latency in some sensible manner.
*** Administrative scalability: 
    adding more nodes should not increase the administrative costs of the system (e.g. the administrators-to-machines ratio).

	Performance[性能]
    is characterized by the amount of useful work accomplished by a computer system compared to the time and resources used. 

*** Short response time/low latency for a given piece of work
*** High throughput (rate of processing work)
*** Low utilization of computing resource(s)

	Latency[延迟]
    The state of being latent; delay, a period between the initiation of something and the occurrence. 
	This definition is pretty cool, because it highlights how latency is really the time between 
    when something happened and the time it has an impact or becomes visible.
    [事件的发生 -- 事件产生实际影响]


	Availability[服务正常]
    the proportion of time a system is in a functioning condition. 
	If a user cannot access the system, it is said to be unavailable. 

	Distributed systems can take a bunch of unreliable components, 
    and build a reliable system on top of them.
    [如何基于一些不够完善的基础组件，去构建一个可靠的系统]

	You can't tolerate faults you haven't considered
    [知道系统可能面对的风险，才能在设计上防范于未然；
     通过充分而深刻的内省，才能在失败的基础上站起来]

	分布式系统所面临的必然风险：
    [物理上] 节点数量，节点之间距离
    [逻辑上] 独立节点的增加导致错误率上升，并提高了管理成本
             节点之间的通信成本上升
			 节点距离导致物理通信延迟上升


    [抽象和建模]
	[分割和备份]
       
	Further reading
    The Datacenter as a Computer - An Introduction to the Design of Warehouse-Scale Machines - Barroso & Hölzle, 2008
    Fallacies of Distributed Computing
    Notes on Distributed Systems for Young Bloods - Hodges, 2013

2. 抽象级别，从高到低
   
   Distributed programming
   finding a good abstraction that balances what is possible with what is understandable and performant.
   [在现实的可能性和人的可理解可实用之间找到平衡，是分布式编程的精髓]


   尼采关于抽象：
   Every concept originates through our equating what is unequal. 
   No leaf ever wholly equals another, and the concept "leaf" is formed through an arbitrary abstraction from these individual differences, 
   through forgetting the distinctions; and now it gives rise to the idea that in nature there might be something besides the leaves which would be "leaf" 
   - some kind of original form after which all leaves have been woven, marked, copied, colored, curled, and painted, but by unskilled hands, 
   so that no copy turned out to be a correct, reliable, and faithful image of the original form.
   [抽象本质上是假的，但有助于我们管理和理解这纷繁复杂的现实世界]

   
   System model[分布式系统的模型]
   a set of assumptions about the environment and facilities on which a distributed system is implemented 

   [系统模型中的节点]   
    the ability to execute a program
    the ability to store data into volatile memory (which can be lost upon failure) and into stable state (which can be read after a failure)
    a clock (which may or may not be assumed to be accurate)

   [系统模型中节点的通信]
   [时/序假设] 
   Synchronous system model [同步模型简单但不现实]
    Processes execute in lock-step; there is a known upper bound on message transmission delay; 
    each process has an accurate clock
   Asynchronous system model
    No timing assumptions - e.g. processes execute at independent rates; 
    there is no bound on message transmission delay; useful clocks do not exist 

   [感知难题]
    集群中所有的节点维护一个共同的价值观：）   
    Agreement: Every correct process must agree on the same value.
    Integrity: Every correct process decides at most one value, and if it decides some value, then it must have been proposed by some process.
    Termination: All processes eventually reach a decision.
    Validity: If all correct processes propose the same value V, then all correct processes decide V.
	

	[两种不可能]
	FLP impossibility: 
    前提：
	assumed that nodes can only fail by crashing; 
	that the network is reliable, and that the typical timing assumptions of the asynchronous system model hold: 
	e.g. there are no bounds on message delay.

	The CAP theorem:
	
    Consistency: all nodes see the same data at the same time.
    Availability: node failures do not prevent survivors from continuing to operate.
    Partition tolerance: the system continues to operate despite message loss due to network and/or node failure
	
	only two can be satisfied simultaneously.
	[要同时到达Consistency, Availability, Partition tolerance的分布式系统是不存在的]

    [衍生模型]
	1. CA (consistency + availability). Examples include full strict quorum protocols, such as two-phase commit.
	2. CP (consistency + partition tolerance). Examples include majority quorum protocols in which minority partitions are unavailable such as Paxos.
    3. AP (availability + partition tolerance). Examples include protocols using conflict resolution, such as Dynamo.

	CA和CP都提供了强一致性保证，CA无法容忍任何一个节点的崩溃，一个有2n+1的CP系统最多可以有n个节点同时崩溃
	1. First, that many system designs used in early distributed relational database systems 
       did not take into account partition tolerance (e.g. they were CA designs). 
	   Partition tolerance is an important property for modern systems, since network partitions become much more 
       likely if the system is geographically distributed (as many large systems are).

    2. Second, that there is a tension between strong consistency and high availability during network partitions.
	3. Third, that there is a tension between strong consistency and performance in normal operation.

    [几个经典的一致性模型]
	Consistency model
    a contract between programmer and system, wherein the system guarantees that if the programmer follows some specific rules, 
	the results of operations on the data store will be predictable 

	[强一致性模型]
    Linearizable consistency: 
	    Under linearizable consistency, all operations appear to have executed atomically in an order that is consistent 
        with the global real-time ordering of operations. (Herlihy & Wing, 1991)
    Sequential consistency: 
	    Under sequential consistency, all operations appear to have executed atomically in some order that is consistent 
        with the order seen at individual nodes and that is equal at all nodes. (Lamport, 1979)

	[其他一致性模型]
	
    Further reading:	
    Brewer's Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services - Gilbert & Lynch, 2002
    Impossibility of distributed consensus with one faulty process - Fischer, Lynch and Patterson, 1985
    Perspectives on the CAP Theorem - Gilbert & Lynch, 2012
    CAP Twelve Years Later: How the "Rules" Have Changed - Brewer, 2012
    Uniform consensus is harder than consensus - Charron-Bost & Schiper, 2000
    Replicated Data Consistency Explained Through Baseball - Terry, 2011
    Life Beyond Distributed Transactions: an Apostate's Opinion - Helland, 2007
    If you have too much data, then 'good enough' is good enough - Helland, 2011
    Building on Quicksand - Helland & Campbell, 2009

3. 事件和顺序
   ? 时间在各处以相同的速度流逝吗？
   1. 全局时钟
   2. 局部时钟
   3. 逻辑时钟

   向量时钟：
   
* 科学上网
** Linux 上sock5转http(s)代理
*** goproxy	
